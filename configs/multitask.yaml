seed: 42

model:
  model_id: openai/whisper-small

dataset:
  audio_dir: ./examples/audio
  transcripts_json: ./examples/transcripts.json
  test_size: 0.1

training:
  # batch & scaling
  output_dir: outputs/whisper-codeswitch
  overwrite_output_dir: true
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 4

  # optimization
  num_train_epochs: 1
  learning_rate: 1.0e-5
  lr_scheduler_type: cosine
  warmup_ratio: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.98
  adam_epsilon: 1.0e-8
  weight_decay: 0.01
  max_grad_norm: 1.0
  label_smoothing_factor: 0.1

  # precision & memory
  fp16: true
  gradient_checkpointing: true

  # evaluation & saving
  evaluation_strategy: steps
  eval_steps: 1000
  save_strategy: steps
  save_steps: 1000
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: wer
  greater_is_better: false

  # logging
  logging_steps: 50
  report_to: none

  # dataloader
  dataloader_num_workers: 4
  remove_unused_columns: false

tokening:
  # inline tags present in dataset (these will be converted to Whisper tokens)
  inline_language_tags: ["[vi]", "[en]"]

multitask:
  task_probabilities:
    transcribe: 0.6
    translate_vi: 0.2
    translate_en: 0.2
